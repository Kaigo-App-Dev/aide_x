"""
æ§‹é€ è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€AIãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ§‹é€ ã®è©•ä¾¡ã‚’è¡Œã†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import logging
from typing import Optional, Dict, Any, TYPE_CHECKING
from src.llm.controller import AIController
if TYPE_CHECKING:
    from src.llm.evaluators.claude_evaluator import ClaudeEvaluator
from src.llm.evaluators.gemini_evaluator import GeminiEvaluator
from src.exceptions import ProviderError, PromptError
from src.llm.controller import controller
from src.llm.prompts.manager import prompt_manager
from src.structure.utils import StructureDict
from src.llm.evaluators.common import EvaluationResult
from src.common.logging_utils import get_logger, log_exception
from src.exceptions import PromptNotFoundError

logger = get_logger(__name__)

def evaluate_structure_with(
    provider_name: str,
    structure: StructureDict,
    model: Optional[str] = None
) -> Optional[EvaluationResult]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’ä½¿ç”¨ã—ã¦æ§‹æˆã‚’è©•ä¾¡ã—ã¾ã™ã€‚

    Args:
        provider_name: ä½¿ç”¨ã™ã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€å (ä¾‹: "claude")
        structure: è©•ä¾¡å¯¾è±¡ã®æ§‹æˆ
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)

    Returns:
        è©•ä¾¡çµæœ (EvaluationResult) ã¾ãŸã¯å¤±æ•—ã—ãŸå ´åˆã¯ None
    """
    logger.info(f"ğŸš€ æ§‹æˆè©•ä¾¡é–‹å§‹: ãƒ—ãƒ­ãƒã‚¤ãƒ€={provider_name}, æ§‹æˆID={structure.get('id')}")
    try:
        provider = controller.get_provider(provider_name)
        if not provider or not hasattr(provider, 'evaluate'):
            logger.error(f"âŒ ãƒ—ãƒ­ãƒã‚¤ãƒ€ '{provider_name}' ã¾ãŸã¯ãã®è©•ä¾¡æ©Ÿèƒ½ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™
        prompt_template_name = f"{provider_name}.structure_evaluation"
        try:
            prompt_manager.get_prompt(provider_name, "structure_evaluation")
        except PromptNotFoundError:
            logger.warning(f"âš ï¸ è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ '{prompt_template_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® 'claude.structure_evaluation' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            prompt_template_name = "claude.structure_evaluation"
        
        prompt = prompt_manager.format_prompt(prompt_template_name, structure=structure)
        logger.debug(f"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸè©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ({prompt_template_name}):\n---\n{prompt}\n---")

        # è©•ä¾¡ã®å®Ÿè¡Œ
        logger.info(f"ğŸ§  {provider_name} ã§è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
        # `evaluate` ãƒ¡ã‚½ãƒƒãƒ‰ã¯ BaseProvider ã§å‹å®šç¾©ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€hasattrã§ãƒã‚§ãƒƒã‚¯
        if hasattr(provider, "evaluate"):
            evaluation_result = provider.evaluate(prompt, model=model)
        else:
            logger.error(f"âŒ ãƒ—ãƒ­ãƒã‚¤ãƒ€ '{provider_name}' ã« 'evaluate' ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return None

        if evaluation_result:
            logger.info(f"âœ… è©•ä¾¡æˆåŠŸ: {evaluation_result}")
        else:
            logger.error("âŒ è©•ä¾¡å¤±æ•—: è©•ä¾¡çµæœãŒNoneã§ã™ã€‚")

        return evaluation_result

    except Exception as e:
        log_exception(logger, e, f"æ§‹æˆè©•ä¾¡å‡¦ç† (ãƒ—ãƒ­ãƒã‚¤ãƒ€: {provider_name})")
        return None 