"""
LLM Module

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

from typing import List, Dict, Any, Optional
from src.llm.providers.base import ChatMessage
from src.types import AIProviderResponse, StructureDict, EvaluationResult
from .hub import LLMHub
from .providers.base import BaseLLMProvider
from .providers.claude import ClaudeProvider
from .providers.chatgpt import ChatGPTProvider
from .providers.gemini import GeminiProvider
from .prompts import prompt_manager, PromptManager
from .controller import AIController
import logging

__all__ = [
    "BaseLLMProvider",
    "ChatMessage",
    "LLMHub",
    "ClaudeProvider",
    "ChatGPTProvider",
    "GeminiProvider",
    "PromptManager",
    "prompt_manager",
    "AIController"
]

logger = logging.getLogger(__name__)

def call_model(
    model: str,
    messages: List[ChatMessage],
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048,
    provider: Optional[str] = None,
    **kwargs
) -> AIProviderResponse:
    """
    LLMã‚’å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’å–å¾—ï¼ˆç°¡æ˜“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰
    
    Args:
        model (str): ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: "claude-3-opus", "gemini-pro"ï¼‰
        messages (List[ChatMessage]): ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
        system_prompt (Optional[str]): ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.0-1.0ï¼‰
        max_tokens (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        provider (Optional[str]): ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åï¼ˆæŒ‡å®šãŒãªã„å ´åˆã¯ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰è‡ªå‹•åˆ¤å®šï¼‰
        
    Returns:
        AIProviderResponse: {
            "content": str,  # å®Ÿéš›ã®ç”Ÿæˆçµæœ
            "raw": Optional[Dict[str, Any]],  # å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨æ–‡ï¼ˆä»»æ„ï¼‰
            "provider": Optional[str],  # "claude", "gemini", "chatgpt" ãªã©
            "error": Optional[str],  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆä»»æ„ï¼‰
            "model": Optional[str],  # ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å
            "usage": Optional[Dict[str, Any]]  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚„ã‚³ã‚¹ãƒˆæƒ…å ±
        }
    """
    logger.info(f"ğŸ” call_modelé–‹å§‹ - model: {model}, provider: {provider}")
    logger.debug(f"call_model - messages: {messages}")
    logger.debug(f"call_model - system_prompt: {system_prompt}")
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰åˆ¤å®š
    if not provider:
        if "claude" in model.lower():
            provider = "claude"
        elif "gemini" in model.lower():
            provider = "gemini"
        elif "gpt" in model.lower():
            provider = "chatgpt"
        else:
            error_msg = f"Unknown model: {model}"
            logger.error(f"âŒ call_model - {error_msg}")
            raise ValueError(error_msg)
    
    logger.info(f"âœ… call_model - ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")

    # ChatMessageã‚’Dict[str, str]ã«å¤‰æ›
    formatted_messages = []
    for msg in messages:
        if hasattr(msg, 'role') and hasattr(msg, 'content'):
            formatted_messages.append({
                "role": msg.role,
                "content": msg.content
            })
        elif isinstance(msg, dict):
            formatted_messages.append({
                "role": msg.get("role", "user"),
                "content": msg.get("content", "")
            })
        else:
            logger.warning(f"âš ï¸ call_model - ç„¡åŠ¹ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼: {msg}")
            continue
    
    logger.debug(f"call_model - formatted_messages: {formatted_messages}")

    try:
        logger.info(f"ğŸš€ call_model - AIController.callé–‹å§‹ - provider: {provider}")
        response = AIController.call(
            provider=provider,
            messages=formatted_messages,
            model=model,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        logger.info(f"âœ… call_model - AIController.callæˆåŠŸ")
        logger.debug(f"call_model - response: {response}")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‹ãƒã‚§ãƒƒã‚¯
        if isinstance(response, dict):
            content = response.get("content", "")
        else:
            content = str(response) if response is not None else ""
        
        logger.info(f"ğŸ“ call_model - æŠ½å‡ºã•ã‚ŒãŸcontenté•·: {len(content)}")
        logger.debug(f"call_model - æŠ½å‡ºã•ã‚ŒãŸcontent: {content[:200]}...")
        
        return {
            "content": content,
            "raw": response if isinstance(response, dict) else None,
            "provider": provider,
            "error": None,
            "model": model,
            "usage": response.get("usage") if isinstance(response, dict) else None
        }
    except Exception as e:
        error_msg = f"call_model - AIController.callå¤±æ•—: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return {
            "content": "",
            "raw": None,
            "provider": provider,
            "error": error_msg,
            "model": model,
            "usage": None
        }

def chat_completion(
    messages: List[ChatMessage],
    model: str = "gpt-3.5-turbo",
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
) -> AIProviderResponse:
    """ChatGPT APIã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆè£œå®Œã‚’å®Ÿè¡Œ"""
    provider = ChatGPTProvider(prompt_manager=prompt_manager)
    response = provider.chat(
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    return {
        "content": response,
        "raw": None,
        "provider": "chatgpt",
        "error": None,
        "model": model,
        "usage": None
    }

"""LLM (Large Language Model) related functionality.""" 