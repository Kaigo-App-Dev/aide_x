"""
AIãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ãƒãƒ–

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€AIãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—ã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ãƒãƒ–ã‚’æä¾›ã—ã¾ã™ã€‚
"""

from typing import Dict, Any, Optional, List, Sequence, cast
from dataclasses import dataclass
import logging
import json
from .controller import AIController
from .providers.base import ChatMessage
from .prompts import prompt_manager, PromptManager
from src.exceptions import AIProviderError, APIRequestError, ResponseFormatError, PromptNotFoundError
from src.types import AIProviderResponse
from datetime import datetime
from src.llm.providers.base import BaseLLMProvider
from src.llm.providers.claude import ClaudeProvider
from src.llm.providers.gemini import GeminiProvider
from src.utils.logging import save_log

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """LLMã®å¿œç­”ã®å®šç¾©"""
    content: str
    error: Optional[str] = None

class LLMHub:
    """LLM Hub - AIãƒ—ãƒ­ãƒã‚¤ãƒ€ã®ä¸€å…ƒç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def call_model(
        model_name: str,
        prompt_name: str,
        prompt_manager: 'PromptManager',
        **kwargs
    ) -> AIProviderResponse:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè¡Œ
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            prompt_name: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå
            prompt_manager: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£
            **kwargs: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ¸¡ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            AIProviderResponse: ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”
        """
        provider = get_provider(model_name)
        if not provider:
            raise AIProviderError(f"Provider not found: {model_name}")
        prompt = prompt_manager.get(model_name, prompt_name)
        if prompt is None:
            raise PromptNotFoundError(f"Prompt not found: {model_name}.{prompt_name}")
        return provider.chat(prompt, model_name, prompt_manager, **kwargs)
    
    @staticmethod
    def chat(
        provider_name: str,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
    ) -> LLMResponse:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒãƒ£ãƒƒãƒˆã‚’å®Ÿè¡Œ
        
        Args:
            provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            messages: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.0-1.0ï¼‰
            max_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            LLMResponse: ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
        """
        return chat(provider_name, messages, temperature, max_tokens)
    
    @staticmethod
    def evaluate_structure(
        provider_name: str,
        structure: Dict[str, Any]
    ) -> LLMResponse:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ§‹æˆã‚’è©•ä¾¡
        
        Args:
            provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            structure: è©•ä¾¡ã™ã‚‹æ§‹æˆ
            
        Returns:
            LLMResponse: è©•ä¾¡çµæœ
        """
        return evaluate_structure(provider_name, structure)
    
    @staticmethod
    def generate_structure(
        provider_name: str,
        requirements: str
    ) -> LLMResponse:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ§‹æˆã‚’ç”Ÿæˆ
        
        Args:
            provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            requirements: ç”Ÿæˆè¦ä»¶
            
        Returns:
            LLMResponse: ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆ
        """
        return generate_structure(provider_name, requirements)

def get_provider(provider_name: str) -> Optional[BaseLLMProvider]:
    """
    ãƒ—ãƒ­ãƒã‚¤ãƒ€åã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹
    
    Args:
        provider_name (str): ãƒ—ãƒ­ãƒã‚¤ãƒ€åï¼ˆ"chatgpt", "claude", "gemini"ï¼‰
        
    Returns:
        Optional[BaseLLMProvider]: ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€å­˜åœ¨ã—ãªã„å ´åˆã¯None
    """
    if provider_name == "chatgpt":
        from .providers.chatgpt import ChatGPTProvider
        return ChatGPTProvider(prompt_manager)
    elif provider_name == "claude":
        return ClaudeProvider(prompt_manager)
    elif provider_name == "gemini":
        return GeminiProvider(prompt_manager)
    return None

def call_model(provider_name: str, model_name: str, prompt_name: str, prompt_manager: 'PromptManager', **kwargs) -> str:
    """
    AIãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’å–å¾—ã™ã‚‹
    Args:
        provider_name (str): ãƒ—ãƒ­ãƒã‚¤ãƒ€å
        model_name (str): ãƒ¢ãƒ‡ãƒ«å
        prompt_name (str): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå
        prompt_manager (PromptManager): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£
        **kwargs: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ¸¡ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    Returns:
        str: ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
    """
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
    logger.info(f"ğŸ” call_model - provider_name: {provider_name}, prompt_name: {prompt_name}")
    logger.info(f"ğŸ” prompt_manager.prompts keys: {list(prompt_manager.prompts.keys())}")
    
    provider = get_provider(provider_name)
    if not provider:
        raise AIProviderError(f"Provider not found: {provider_name}")
    
    prompt = prompt_manager.get_prompt(provider_name, prompt_name)
    if prompt is None:
        logger.error(f"âŒ Prompt not found - provider: {provider_name}, prompt_name: {prompt_name}")
        logger.error(f"âŒ Available prompts: {list(prompt_manager.prompts.keys())}")
        raise PromptNotFoundError(provider_name, prompt_name)
    
    logger.info(f"âœ… Prompt found: {prompt_name}")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    try:
        logger.info(f"ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ - kwargs: {kwargs}")
        if 'structure' in kwargs and isinstance(kwargs['structure'], str):
            logger.info(f"ğŸ” structureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¤œå‡º: {len(kwargs['structure'])} æ–‡å­—")
        formatted_content = prompt.format(**kwargs)
        logger.info(f"âœ… Prompt formatted successfully")
        logger.info(f"ğŸ” ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµæœï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {formatted_content[:200]}...")
        logger.debug(f"Claudeæœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨æ–‡:\n{formatted_content}")
    except Exception as e:
        logger.error(f"âŒ Prompt formatting failed: {str(e)}")
        logger.error(f"âŒ kwargs: {kwargs}")
        logger.error(f"âŒ prompt template: {prompt.template[:200]}...")
        raise PromptNotFoundError(provider_name, prompt_name)
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
    messages = [ChatMessage(role="user", content=formatted_content)]
    
    # chatãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
    if provider_name == "claude":
        # ClaudeProviderã¯ç•°ãªã‚‹å¼•æ•°å½¢å¼ã‚’æœŸå¾…
        return provider.chat(prompt, model_name, prompt_manager)
    else:
        # ãã®ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯æ¨™æº–çš„ãªå½¢å¼
        return provider.chat(messages, prompt_manager)

def chat(
    provider_name: str,
    messages: List[ChatMessage],
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> LLMResponse:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒãƒ£ãƒƒãƒˆã‚’å®Ÿè¡Œ
    
    Args:
        provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
        messages: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
        temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.0-1.0ï¼‰
        max_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        
    Returns:
        LLMResponse: ç”Ÿæˆã•ã‚ŒãŸå¿œç­”
    """
    try:
        response = AIController.call(
            provider=provider_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        return LLMResponse(content=response.get("content", ""))
        
    except (APIRequestError, ResponseFormatError) as e:
        logger.error(f"Error in chat with {provider_name}: {str(e)}")
        return LLMResponse(content="", error=str(e))
    except Exception as e:
        logger.error(f"Unexpected error in chat with {provider_name}: {str(e)}")
        return LLMResponse(content="", error=str(e))

def evaluate_structure(
    provider_name: str,
    structure: Dict[str, Any]
) -> LLMResponse:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ§‹æˆã‚’è©•ä¾¡
    
    Args:
        provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
        structure: è©•ä¾¡ã™ã‚‹æ§‹æˆ
        
    Returns:
        LLMResponse: è©•ä¾¡çµæœ
    """
    try:
        prompt_manager.register_builtin_templates()
        # TODO: Unused template - review/delete - evaluation_template is not registered in templates
        evaluation_template = prompt_manager.get_prompt(provider_name, "evaluation_template")
        if not evaluation_template:
            raise ValueError(f"Evaluation template not found for {provider_name}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        prompt = evaluation_template.render(
            title=structure.get("title", ""),
            description=structure.get("description", ""),
            content=structure.get("content", {})
        )
        
        messages = [
            ChatMessage(role="user", content=prompt)
        ]
        
        response = AIController.call(
            provider=provider_name,
            messages=messages
        )
        
        return LLMResponse(content=response.get("content", ""))
        
    except (APIRequestError, ResponseFormatError) as e:
        logger.error(f"Error in structure evaluation with {provider_name}: {str(e)}")
        return LLMResponse(content="", error=str(e))
    except Exception as e:
        logger.error(f"Unexpected error in structure evaluation with {provider_name}: {str(e)}")
        return LLMResponse(content="", error=str(e))

def generate_structure(
    provider_name: str,
    requirements: str
) -> LLMResponse:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§æ§‹æˆã‚’ç”Ÿæˆ
    
    Args:
        provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
        requirements: ç”Ÿæˆè¦ä»¶
        
    Returns:
        LLMResponse: ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆ
    """
    try:
        prompt_manager.register_builtin_templates()
        # TODO: Unused template - review/delete - generation_template is not registered in templates
        generation_template = prompt_manager.get_prompt(provider_name, "generation_template")
        if not generation_template:
            raise ValueError(f"Generation template not found for {provider_name}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        prompt = generation_template.render(requirements=requirements)
        
        messages = [
            ChatMessage(role="user", content=prompt)
        ]
        
        response = AIController.call(
            provider=provider_name,
            messages=messages
        )
        
        return LLMResponse(content=response.get("content", ""))
        
    except (APIRequestError, ResponseFormatError) as e:
        logger.error(f"Error in structure generation with {provider_name}: {str(e)}")
        return LLMResponse(content="", error=str(e))
    except Exception as e:
        logger.error(f"Unexpected error in structure generation with {provider_name}: {str(e)}")
        return LLMResponse(content="", error=str(e))

def safe_generate_and_evaluate(chat_history: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    å®‰å…¨ã«æ§‹æˆã‚’ç”Ÿæˆã—ã¦è©•ä¾¡ã™ã‚‹
    
    Args:
        chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´
        
    Returns:
        Dict[str, Any]: ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆã¨è©•ä¾¡çµæœ
    """
    try:
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‹ã‚‰è¦ä»¶ã‚’æŠ½å‡º
        requirements = "\n".join([
            msg["content"] for msg in chat_history
            if msg["role"] == "user"
        ])
        
        # æ§‹æˆã‚’ç”Ÿæˆ
        response = generate_structure("claude", requirements)
        if response.error:
            raise ValueError(f"Generation failed: {response.error}")
        
        # ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆã‚’ãƒ‘ãƒ¼ã‚¹
        try:
            structure = json.loads(response.content)
        except json.JSONDecodeError:
            raise ValueError("Failed to parse generated structure")
        
        # æ§‹æˆã‚’è©•ä¾¡
        eval_response = evaluate_structure("claude", structure)
        if eval_response.error:
            raise ValueError(f"Evaluation failed: {eval_response.error}")
        
        # è©•ä¾¡çµæœã‚’ãƒ‘ãƒ¼ã‚¹
        try:
            evaluation = json.loads(eval_response.content)
        except json.JSONDecodeError:
            raise ValueError("Failed to parse evaluation result")
        
        return {
            "structure": structure,
            "evaluation": evaluation
        }
        
    except Exception as e:
        logger.error(f"Error in safe_generate_and_evaluate: {str(e)}")
        return {
            "structure": None,
            "evaluation": None,
            "error": str(e)
        }

def normalize_structure_format(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ã•ã‚ŒãŸæ§‹é€ å½¢å¼ã«å¤‰æ›
    
    Args:
        raw_data: ç”Ÿã®ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict[str, Any]: æ­£è¦åŒ–ã•ã‚ŒãŸæ§‹é€ 
    """
    try:
        # åŸºæœ¬çš„ãªæ§‹é€ ã‚’ç¢ºä¿
        structure = {
            "title": raw_data.get("title", "Untitled"),
            "description": raw_data.get("description", ""),
            "content": raw_data.get("content", {}),
            "metadata": raw_data.get("metadata", {}),
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # contentãŒæ–‡å­—åˆ—ã®å ´åˆã¯JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        if isinstance(structure["content"], str):
            try:
                structure["content"] = json.loads(structure["content"])
            except json.JSONDecodeError:
                structure["content"] = {"raw": structure["content"]}
        
        return structure
        
    except Exception as e:
        logger.error(f"Error in normalize_structure_format: {str(e)}")
        return {
            "title": "Error",
            "description": f"Failed to normalize structure: {str(e)}",
            "content": {"error": str(e)},
            "metadata": {},
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        } 